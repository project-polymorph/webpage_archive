# 禁止算法识别性别，能消解歧视吗？

![图片1](https://img2.jiemian.com/jiemian/original/20210420/161893159541235700_a300x300.jpg)

**来源**：界面财经号  
**作者**：脑极体  
**时间**：2021年04月21日 07:44  
**IP属地**：北京

让算法判断男女这件事，真的很危险吗？

海外科技圈的政治正确风潮，早已不算新闻了，并且已经遍布科技领域的各个角度。

Facebook将默认“男性在前”的新增好友logo，改为“女性在前”，并且大小相当。还提供了除男、女之外的非传统性别供用户选择，比如无性别、双性人、跨性别等等，恨不得将“性别平等”四个大字贴在脑门儿上；

在日常管理中，科技公司对政治不正确的言论也保持着十足的警惕。比如谷歌就曾因一位工程师散布“有害无益的性别主义成见”而将其开除。

在对可能的冒犯充满审慎的大环境下，一些反面声音也出现了。

《纽约时报》就曾发文称“政治正确是美国精英阶层的虚伪”，也有业内人士认为“不合逻辑、肆无忌惮的政治正确正在阻碍科技创新”等等。最直接的例子，就是IBM宣布退出了人脸识别技术的研发，原因是为了消除种族歧视，因为目前的算法无法对黑人面部做出精准判断。

![图片2](https://img2.jiemian.com/jiemian/original/20210420/161893159541235700_a320xH.jpg)

事实上，算法对于少数群体的偏见，已经成为困扰科技界的一大难题。

目前，欧盟的数字保护组织掀起了一项新的运动，要求像禁止在城市大规模部署面部识别一样，禁止利用算法来主动预测某人的性别和性取向。

用算法判断男女这件事，为什么让欧洲人觉得难以接受？对此的抗议是对科技伦理的合理讨论，还是过度政治正确下对科技创新的掣肘呢？

#### 性别二元论的算法，真的危险吗？

对于人工智能新技术，欧盟有许多不可承受之重。不能接受个人数据被滥用，有了史上最严的隐私保护法案GDPR；为了防止技术风险，计划五年内公共场所禁用人脸识别。

现在，为了避免歧视与偏见，连算法对男、女性别的识别，欧盟也开始拒绝了。

如果说对黑人面部的识别不到位是源自于机器视觉技术的天然缺陷，那么如果禁止自动性别识别，可能就直接把既定的社会规则在算法世界的投射给掐灭了。

比如现实中，如果你是一家便利店的老板，见到顾客的第一反应就能识别出对方的生理性别是男是女，从而判断出自己所在商圈的性别分布概率。但现在，欧洲的运动发起者们则希望算法不要采集并分析性别数据。

那么，让算法判断男女这件事，真的很危险吗？

首先，性别识别算法的判断基于二元论，即男或女，无法涵盖多元人群的性别表达。

在数据上，除了Facebook这类深受“政治正确”影响的平台，绝大多数数据标注公司都会采用性别二元论，依照传统的生物特征和性别理解，来为人脸进行标注。比如短发就归类为男人，化妆就归类为女人。这就导致许多变性人和跨性别者，会直接被误认。从而在一些以性别为基础的活动中遭受不公正的待遇，比如此前柏林为庆祝同工同酬日，会向女性提供打折地铁票，就是使用面部扫描来识别女性，这显然会让一些跨性别者无法被覆盖。

其次，数字线索的失真会直接影响算法的有效性，从而丧失预期的商业价值。

性别识别算法被用来做什么？一般都脱离不了商业目的或公共安全，比如可以通过识别男女来缩小追踪嫌疑人的范围，分析某人的声音并汇总他们的购物习惯来提升电商平台的推荐算法。亚马逊等大型科技公司销售的商业系统中，性别分类都是标配。

但现实中机灵的商家能根据一个人的声情行表判断出对方的大致偏好或心理性别，进而提供个性化服务。在数字世界中，这些特征被简化为二元结构的时候，不可能达成预期的准确度。

比如斯坦福大学研发的“AI gaydar”系统，号称可以 81％的准确率来识别男同性恋，但遭到了来自普林斯顿大学、谷歌等多位研究人员的质疑，认为其研究存在严重缺陷，模型并没有发现不同性取向的人面部特征有何差异。

显然，部署这样的算法不仅会因为失误而让当事人感到冒犯，也无法给应用者带来有益的商业价值。

科技媒体《theverge》就曾报道过一个“仅限女孩”的社交应用Giggle，试图用面部识别来验证用户身份。如果算法失误，不仅会失去一个新用户，恐怕还会引发舆论指责。

![图片3](https://img2.jiemian.com/jiemian/original/20210420/161893159572605300_a700xH.jpg)

出于担忧，数字权利组织Access Now与其他60多个非政府组织一起，要求欧盟委员会禁止自动性别识别这项技术。

听到这里，大部分人可能已经有点迷惑了，毕竟作为消费者或用户，在商业社会中一个人被全方位地审视和判断，早就是心照不宣的事儿了，且不管有没有用，就算被一个算法认错性别，很重要吗？直接禁止会不会矫枉过正了？

#### 当伪君子遇上真小人

必须承认，自动性别识别在现实中的威胁并没有那么大，对其的抗拒，很大程度上确实是受“实现所有类型的人平等”的政治正确形态影响。

发起机构之一Access Now的员工就表示，这种性别识别技术目前的应用主要是有针对性的广告，比如某一性别专用的App、广告投放，未来还可能限制进入浴室/更衣室等区域。错误的判定结果就可能导致歧视，比如给男性对象投放汽车广告，给女性用户投放连衣裙广告。

试想一下，未来广告推荐算法都不能判断用户的性别，对大家投放一致的广告，是不是就是一个更好的社会呢？

冷静下来想，似乎并不能成立。

这也是科技领域政治正确被反对者广为批判的原因：

1.以科技研发为代价，却仅仅消除了口头歧视，而未能改变根本矛盾。

除了IBM停止人脸识别技术之外，亚马逊也关停了给简历打分的AI算法，原因是通过识别性别关键词而给女性应聘者更低的分数。那么，停用算法，避免了公然的系统偏见，是不是就为黑人或女性创造了更平等的环境呢？显然没有直接关系。

男女在高管职位上的悬殊比例，不会因为把“chairman”改为中性词“chairperson”而消失。

如果政治正确的倡议和运动没有撼动资本制度的能量，那么只是沦为少数群体的发泄途径，而掌握着主控权和选择权的依然是资本。

2.当政治正确延伸到微侵略性领域，过度保护进一步延缓创新。

包容多元群体原本是一件符合人类基本道德观的好事，可是当被用于批判一些微侵略事件——即对任何边缘化或少数群体有意或无意地表示负面偏见的评论，如果一个男性以傲慢的态度向女性解释一些简单的东西，就会被指责为“爹味”、“普且信”，引起摩擦。

过于泛化的政治正确，会带来更多的身份冲突，导致依赖团队协作的科技创新，群体之间的合作变得更加困难。

一位美国工程师就透露，由于希望证明女性和男性一样擅长工程，她有意识地避免在会议上坐在其他女性旁边，尽量自己解决问题，避免寻求帮助。

显然，在受政治正确约束的文化中，自我保护变得比创造性工作更加重要，不仅优势人群如白人男性担心被指责、不敢直接解决问题，那么被过度保护的群体也可能与潜在的支持隔离开来，变得效率更低，技能提升速度也更慢。

如果说公然的歧视与偏见是“真小人”，那么过度的政治正确又容易沦为“伪君子”，被视作一种无法真正解决问题的伪善。

那么，有没有第三种可能，建立一个“真君子”的科技世界呢？

#### 寻找第三种可能

如果说当下的数据隐私保护运动，是为了不让科技的潜在缺陷伤害少数群体或边缘人群。但显然，一个理想的数字世界，并不会因此自动到来，除了“科技不能做什么”，或许还应该主动去思考“科技能够为一个更包容、多元的社会做什么”。

举个最简单的例子，谷歌输入大量“穿白大褂、带听诊器”的男性照片，让AI将医生形象与男性联系在一起。从统计学的角度，算法形成偏见似乎情有可原，因为一直以来都是这样运作的，但比起政治正确的制衡与争议，从根源上改变少数群体在数字世界的“失声”局面，或许更具有建设性。

我们知道，算法对黑人、女性等的偏见，一个最主要的原因就是训练语料的缺失。

数据显示，全世界有42%，也就是11亿的女性都没有银行账户，这自然会导致金融算法对女性打出更低的分级，让女性比男性更难从银行中借贷。所以在涉及金融科技产品时，更应该主动将女性需求纳入到产品和服务当中，比如考虑到女性的风险偏好，帮助缩小两性在投资领域的差距。

再比如，今天科技领域普遍存在的关键工程岗位的男女性别比例失衡问题。在努力改变性别定势思维与偏见的同时，或许察觉到女性在STEAMD学科，即科学、技术、工程、艺术和数学领域上过早的限制，才是缩小未来性别差距的真正方式，也能为IT、AI等领域带来源源不断的新生力量。

从根本上建立起相关支持体系，鼓励少数群体实现自身发展，才是铲除AI偏见、建立多元化社会的起点。

当下的AI是一个天生的偏见者，只有追溯到它出生的地方去寻求改变，一个天下大同的理想数字世界才会真正走来。